[33mcommit b0606cf45fc3ca65487dd6fd39e115b3114d9659[m[33m ([m[1;36mHEAD -> [m[1;32mproto_torchtext[m[33m, [m[1;31morigin/proto_torchtext[m[33m)[m
Author: daniel <Daniel.serna.olarte@gmail.com>
Date:   Wed Dec 22 02:14:47 2021 -0500

    wip

[1mdiff --git a/TTCBin2.py b/TTCBin2.py[m
[1mindex 2c431a6..2c00a5c 100644[m
[1m--- a/TTCBin2.py[m
[1m+++ b/TTCBin2.py[m
[36m@@ -56,7 +56,7 @@[m [mwandb.login()[m
 BERT_MODEL_NAME = 'seyonec/BPE_SELFIES_PubChem_shard00_120k'[m
 tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)[m
 MAX_SEQ_LEN = 256[m
[31m-BATCH_SIZE = 32[m
[32m+[m[32mBATCH_SIZE = 16[m
 [m
 PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)[m
 UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)[m
[1mdiff --git a/utilityFunctions.py b/utilityFunctions.py[m
[1mindex c793b7d..8c08034 100644[m
[1m--- a/utilityFunctions.py[m
[1m+++ b/utilityFunctions.py[m
[36m@@ -2,13 +2,13 @@[m [mimport torch[m
 from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForMaskedLM[m
 [m
 def save_checkpoint(path, model, valid_loss):[m
[31m-    torch.save({'model_state_dict': model.state_dict(),[m
[32m+[m[32m    torch.save({'model_state_dict': model.module.state_dict(),[m
                   'valid_loss': valid_loss}, path)[m
 [m
 [m
 def load_checkpoint(path, model):[m
     state_dict = torch.load(path)[m
[31m-    model.load_state_dict(state_dict['model_state_dict'])[m
[32m+[m[32m    model.module.load_state_dict(state_dict['model_state_dict'])[m
 [m
     return state_dict['valid_loss'][m
 [m
